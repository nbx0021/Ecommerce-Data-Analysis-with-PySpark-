{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7beda82c-dbe6-451e-a01f-84628d0e0945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "\n",
    "# 1. Get the location of your Repo folder\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f985a541-4cc1-4b24-ba40-89c5852e5215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "customer data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd2c943-3018-4633-97e0-e490a1e0cf3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_unique_id\", StringType(), True),\n",
    "    StructField(\"customer_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_state\", StringType(), True)\n",
    "])\n",
    "csv_path = f\"file:{current_dir}/data/olist_customers_dataset.csv\"\n",
    "\n",
    "# df_customers = spark.read.schema(customer_schema).table(\"workspace.ecommerce.olist_customers_dataset\")\n",
    "# print(f\"Reading file from: {csv_path}\")\n",
    "\n",
    "# 2. Read using PANDAS (This bypasses the Security Exception)\n",
    "# We treat all columns as strings first to match your Spark Schema logic\n",
    "pdf_customers = pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "# 3. Convert Pandas DataFrame -> Spark DataFrame\n",
    "# df_customers = spark.createDataFrame(pdf_customers)\n",
    "\n",
    "# 4. Enforce your specific schema (Optional, but good practice)\n",
    "# Since we read everything as string, we can cast it now or just apply schema\n",
    "df_customers = spark.createDataFrame(pdf_customers, schema=customer_schema)\n",
    "\n",
    "\n",
    "display(df_customers.head(5))\n",
    "# display(df_customers.describe())\n",
    "is_null_customer = {col_name: df_customers.filter(df_customers[col_name].isNull()).count() for col_name in df_customers.columns}\n",
    "percentage_of_null_customer = {col_name: (df_customers.filter(df_customers[col_name].isNull()).count() / df_customers.count()) * 100 for col_name in df_customers.columns}\n",
    "\n",
    "df_customers.printSchema()\n",
    "\n",
    "display({col_name: f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_customer.items()})\n",
    "print(\"\\n\")\n",
    "display(is_null_customer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af64270b-1361-448d-93b5-b7848d80de86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "order data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52fc27c6-e17a-4df1-9adf-d5f700d58c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_status\",StringType(),True),\n",
    "    StructField(\"order_purchase_timestamp\", StringType(), True),\n",
    "    StructField(\"order_approved_at\", StringType(), True),\n",
    "    StructField(\"order_delivered_carrier_date\", StringType(), True),\n",
    "    StructField(\"order_delivered_customer_date\", StringType(), True),\n",
    "    StructField(\"order_estimated_delivery_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_path = f\"file:{current_dir}/data/olist_orders_dataset.csv\"\n",
    "pdf_orders = pd.read_csv(csv_path, dtype=str)\n",
    "df_orders = spark.createDataFrame(pdf_orders, schema=order_schema)\n",
    "\n",
    "df_orders.printSchema()\n",
    "\n",
    "display(df_orders.head(5))\n",
    "display(df_orders.describe())\n",
    "is_null_order = {col_name: df_orders.filter(df_orders[col_name].isNull()).count() for col_name in df_orders.columns}\n",
    "percentage_of_null_order = {col_name: (df_orders.filter(df_orders[col_name].isNull()).count() / df_orders.count()) * 100 for col_name in df_orders.columns}\n",
    "\n",
    "display({col_name: f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_order.items()})\n",
    "print(\"\\n\")\n",
    "display(is_null_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981b6d81-df6f-40e5-a659-5ab8e581b00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "order items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c661e34c-2da3-4c58-8ba2-0ed1acb0ce1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "order_items_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_item_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"shipping_limit_date\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"freight_value\", DoubleType(), True)\n",
    "])\n",
    "csv_path = f\"file:{current_dir}/data/olist_order_items_dataset.csv\"\n",
    "pdf_order_items = pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "# Minimal fix: convert price and freight_value to float\n",
    "pdf_order_items[\"price\"] = pd.to_numeric(pdf_order_items[\"price\"], errors=\"coerce\")\n",
    "pdf_order_items[\"freight_value\"] = pd.to_numeric(pdf_order_items[\"freight_value\"], errors=\"coerce\")\n",
    "\n",
    "df_order_items = spark.createDataFrame(pdf_order_items, schema=order_items_schema)\n",
    "\n",
    "# df_order_items = spark.read.schema(order_items_schema).table(\"workspace.ecommerce.olist_order_items_dataset\")\n",
    "df_order_items.printSchema()\n",
    "display(df_order_items.head(5))\n",
    "is_null_order_items = {col_name: df_order_items.filter(df_order_items[col_name].isNull()).count() for col_name in df_order_items.columns}\n",
    "percentage_of_null_order_items = {col_name: (df_order_items.filter(df_order_items[col_name].isNull()).count() / df_order_items.count()) * 100 for col_name in df_order_items.columns}\n",
    "\n",
    "display({col_name: f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_order_items.items()})\n",
    "print(\"\\n\")\n",
    "display(is_null_order_items)\n",
    "display(df_order_items.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a237b2-c342-41ce-842c-f27d7d79d5e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "order payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0810f4bb-0019-498f-bcde-4e308a01029b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType,IntegerType\n",
    "\n",
    "order_payments_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_sequential\", StringType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"payment_installments\", IntegerType(), True),\n",
    "    StructField(\"payment_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "csv_path = f\"file:{current_dir}/data/olist_order_payments_dataset.csv\"\n",
    "pdf_orders_payments = pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "pdf_orders_payments[\"payment_installments\"] = pd.to_numeric(pdf_orders_payments[\"payment_installments\"], errors=\"coerce\")\n",
    "pdf_orders_payments[\"payment_value\"] = pd.to_numeric(pdf_orders_payments[\"payment_value\"], errors=\"coerce\")\n",
    "\n",
    "df_order_payments = spark.createDataFrame(pdf_orders_payments, schema=order_payments_schema)\n",
    "\n",
    "\n",
    "# df_order_payments = spark.read.schema(order_payments_schema).table(\"workspace.ecommerce.olist_order_payments_dataset\")\n",
    "df_order_payments.printSchema()\n",
    "\n",
    "display(df_order_payments.head(5))\n",
    "is_null_order_payments = {col_name: df_order_payments.filter(df_order_payments[col_name].isNull()).count() for col_name in df_order_payments.columns}\n",
    "percentage_of_null_order_payments = {col_name: (df_order_payments.filter(df_order_payments[col_name].isNull()).count() / df_order_payments.count() * 100) for col_name in df_order_payments.columns}\n",
    "display(is_null_order_payments)\n",
    "print(\"\\n\")\n",
    "display({col_name:f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_order_payments.items()})\n",
    "display(df_order_payments.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a2dd33f-60a0-46e7-95d0-ce6c3c405603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "order reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b984f6-787d-4d40-8f7c-2ccdaf16e3f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 10"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "order_reviews_schema = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"review_score\", IntegerType(), True),\n",
    "    StructField(\"review_comment_title\", StringType(), True),\n",
    "    StructField(\"review_comment_message\", StringType(), True),\n",
    "    StructField(\"review_creation_date\", StringType(), True),\n",
    "    StructField(\"review_answer_timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_path = f\"file:{current_dir}/data/olist_order_reviews_dataset.csv\"\n",
    "pdf_orders_reviews = pd.read_csv(csv_path, dtype=str)\n",
    "pdf_orders_reviews[\"review_score\"] = pd.to_numeric(pdf_orders_reviews[\"review_score\"], errors=\"coerce\")\n",
    "df_order_reviews = spark.createDataFrame(pdf_orders_reviews, schema=order_reviews_schema)\n",
    "\n",
    "\n",
    "# df_order_reviews = spark.read.schema(order_reviews_schema).table(\"workspace.ecommerce.olist_order_reviews_dataset\")\n",
    "df_order_reviews.printSchema()\n",
    "\n",
    "display(df_order_reviews.limit(5))\n",
    "is_null_order_reviews = {col_name: df_order_reviews.filter(df_order_reviews[col_name].isNull()).count() for col_name in df_order_reviews.columns}\n",
    "percentage_of_null_order_reviews = {col_name: (df_order_reviews.filter(df_order_reviews[col_name].isNull()).count() / df_order_reviews.count() * 100) for col_name in df_order_reviews.columns}\n",
    "display(is_null_order_reviews)\n",
    "print(\"\\n\")\n",
    "display({col_name:f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_order_reviews.items()})\n",
    "display(df_order_reviews.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e859ed64-3c68-4f57-9d12-3da595d9cfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96ac5fd-40e6-48ab-a740-51ed1a3cadbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_category_name\", StringType(), True),\n",
    "    StructField(\"product_name_lenght\", IntegerType(), True),\n",
    "    StructField(\"product_description_lenght\", IntegerType(), True),\n",
    "    StructField(\"product_photos_qty\", IntegerType(), True),\n",
    "    StructField(\"product_weight_g\", DoubleType(), True),\n",
    "    StructField(\"product_length_cm\", DoubleType(), True),\n",
    "    StructField(\"product_height_cm\", DoubleType(), True),\n",
    "    StructField(\"product_width_cm\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "csv_path = f\"file:{current_dir}/data/olist_products_dataset.csv\"\n",
    "pdf_products = pd.read_csv(csv_path, dtype=str)\n",
    "pdf_products[\"product_name_lenght\"] = pd.to_numeric(pdf_products[\"product_name_lenght\"], errors=\"coerce\")\n",
    "pdf_products[\"product_description_lenght\"] = pd.to_numeric(pdf_products[\"product_description_lenght\"], errors=\"coerce\")\n",
    "pdf_products[\"product_photos_qty\"] = pd.to_numeric(pdf_products[\"product_photos_qty\"], errors=\"coerce\")\n",
    "pdf_products[\"product_weight_g\"] = pd.to_numeric(pdf_products[\"product_weight_g\"], errors=\"coerce\")\n",
    "pdf_products[\"product_length_cm\"] = pd.to_numeric(pdf_products[\"product_length_cm\"], errors=\"coerce\")\n",
    "pdf_products[\"product_height_cm\"] = pd.to_numeric(pdf_products[\"product_height_cm\"], errors=\"coerce\")\n",
    "pdf_products[\"product_width_cm\"] = pd.to_numeric(pdf_products[\"product_width_cm\"], errors=\"coerce\")\n",
    "df_products = spark.createDataFrame(pdf_products, schema=products_schema)\n",
    "\n",
    "# df_products = spark.read.schema(products_schema).table(\"workspace.ecommerce.olist_products_dataset\")\n",
    "df_products.printSchema()\n",
    "display(df_products.limit(5))\n",
    "is_null_products = {col_name: df_products.filter(df_products[col_name].isNull()).count() for col_name in df_products.columns}\n",
    "percentage_of_null_products = {col_name: (df_products.filter(df_products[col_name].isNull()).count() / df_products.count() * 100) for col_name in df_products.columns}\n",
    "display(is_null_products)\n",
    "print(\"\\n\")\n",
    "display({col_name:f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_products.items()})\n",
    "display(df_products.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee684fff-680e-45f7-9b4a-c42d88dc6bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "sellers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94840f1c-a76b-4a8d-a10d-a51e88ea2a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import  sum as spark_sum, when,count as spark_count,col as spark_col\n",
    "import builtins\n",
    "sellers_schema = StructType([\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"seller_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"seller_city\", StringType(), True),\n",
    "    StructField(\"seller_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_path = f\"file:{current_dir}/data/olist_sellers_dataset.csv\"\n",
    "pdf_sellers = pd.read_csv(csv_path, dtype=str)\n",
    "df_sellers = spark.createDataFrame(pdf_sellers, schema=sellers_schema)\n",
    "\n",
    "# df_sellers = spark.read.schema(sellers_schema).table(\"workspace.ecommerce.olist_sellers_dataset\")\n",
    "df_sellers.printSchema()\n",
    "\n",
    "display(df_sellers.limit(5))\n",
    "# for databricks storage\n",
    "# is_null_sellers = {col_name: df_sellers.filter(df_sellers[col_name].isNull()).count() for col_name in df_sellers.columns}\n",
    "# percentage_of_null_sellers = {col_name: round((count/df_sellers.count())*100, 2) for col_name, count in is_null_sellers.items()}\n",
    "\n",
    "total_rows = df_sellers.count()\n",
    "\n",
    "null_counts = df_sellers.select([\n",
    "    spark_count(when(spark_col(c).isNull(), 1)).alias(c)\n",
    "    for c in df_sellers.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# percentage_of_null_sellers = {\n",
    "#     c: round((null_counts[c] / total_rows) * 100, 2)\n",
    "#     for c in null_counts\n",
    "# }\n",
    "\n",
    "# percentage_of_null_sellers = {\n",
    "#     col_name: round((count / total_rows) * 100, 2)\n",
    "#     for col_name, count in null_counts.items()\n",
    "# }\n",
    "\n",
    "\n",
    "percentage_of_null_sellers = {\n",
    "    c: builtins.round((null_counts[c] / total_rows) * 100, 2)\n",
    "    for c in null_counts\n",
    "}\n",
    "\n",
    "display(null_counts)\n",
    "\n",
    "print(\"\\n\")\n",
    "display({col_name:f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_sellers.items()})\n",
    "\n",
    "display(df_sellers.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2199bc21-c2f5-4397-9bd9-3449722452cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "geo location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70fe671-5e90-46a5-8dee-0ec888058bde",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Geolocation Data Load and Null Analysis"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "geolocation_schema = StructType([\n",
    "    StructField(\"geolocation_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"geolocation_lat\", DoubleType(), True),\n",
    "    StructField(\"geolocation_lng\", DoubleType(), True),\n",
    "    StructField(\"geolocation_city\", StringType(), True),\n",
    "    StructField(\"geolocation_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_path = f\"file:{current_dir}/data/olist_geolocation_dataset.csv\"\n",
    "pdf_geolocation = pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "# Minimal fix: convert geolocation_lat and geolocation_lng to float\n",
    "pdf_geolocation[\"geolocation_lat\"] = pd.to_numeric(pdf_geolocation[\"geolocation_lat\"], errors=\"coerce\")\n",
    "pdf_geolocation[\"geolocation_lng\"] = pd.to_numeric(pdf_geolocation[\"geolocation_lng\"], errors=\"coerce\")\n",
    "\n",
    "df_geolocation = spark.createDataFrame(pdf_geolocation, schema=geolocation_schema)\n",
    "\n",
    "# df_geolocation = spark.read.schema(geolocation_schema).table(\"workspace.ecommerce.olist_geolocation_dataset\")\n",
    "df_geolocation.printSchema()\n",
    "display(df_geolocation.limit(5))\n",
    "is_null_geolocation = {col_name: df_geolocation.filter(df_geolocation[col_name].isNull()).count() for col_name in df_geolocation.columns}\n",
    "percentage_of_null_geolocation = {col_name: (df_geolocation.filter(df_geolocation[col_name].isNull()).count() / df_geolocation.count() * 100) for col_name in df_geolocation.columns}\n",
    "display(is_null_geolocation)\n",
    "print(\"\\n\")\n",
    "display({col_name:f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_geolocation.items()})\n",
    "display(df_geolocation.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ae9342-fe82-4c93-a29c-7898739dbdc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "products category translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a63efb-2671-4fbc-a754-250519c63cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "products_category_schema = StructType([\n",
    "    StructField(\"product_category_name\", StringType(), True),\n",
    "    StructField(\"product_category_name_english\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_path = f\"file:{current_dir}/data/product_category_name_translation.csv\"\n",
    "pdf_products_category = pd.read_csv(csv_path, dtype=str)\n",
    "df_products_category = spark.createDataFrame(pdf_products_category, schema=products_category_schema)\n",
    "\n",
    "# df_products_category = spark.read.schema(products_category_schema).table(\"workspace.ecommerce.product_category_name_translation\")\n",
    "df_products_category.printSchema()\n",
    "display(df_products_category.limit(5))\n",
    "is_null_products_category = {col_name: df_products_category.filter(df_products_category[col_name].isNull()).count() for col_name in df_products_category.columns}\n",
    "percentage_of_null_products_category = {col_name: (df_products_category.filter(df_products_category[col_name].isNull()).count() / df_products_category.count() * 100) for col_name in df_products_category.columns}\n",
    "display(is_null_products_category)\n",
    "print(\"\\n\")\n",
    "display({col_name:f\"{percentage:.2f} %\" for col_name, percentage in percentage_of_null_products_category.items()})\n",
    "display(df_products_category.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463446be-e597-492e-b70f-818922c040bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 19"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def indian_number_format(num):\n",
    "    try:\n",
    "        num = float(num)\n",
    "        if num < 1000:\n",
    "            return f\"{num:.2f}\"\n",
    "        elif num < 100000:\n",
    "            return f\"{num/1000:.2f} Thousand\"\n",
    "        elif num < 10000000:\n",
    "            return f\"{num/100000:.2f} Lakh\"\n",
    "        else:\n",
    "            return f\"{num/10000000:.2f} Crore\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "indian_format_udf = udf(indian_number_format, StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bd75593-8557-4c97-b214-e469c95581c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Handling Nulls in Orders (df_orders)**\n",
    "\n",
    "The Problem: You have nulls in order_delivered_customer_date (2.98%) and order_approved_at (0.16%).\n",
    "\n",
    "The Logic: If a delivery date is missing, the order might still be in progress or canceled.\n",
    "\n",
    "The Fix:\n",
    "\n",
    "For Delivered analysis: Drop rows where the delivery date is null.\n",
    "\n",
    "For Funnel analysis: Keep them but flag them as \"Undelivered\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3191c27c-8384-409c-a427-687592f1a6fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, datediff, round,count\n",
    "\n",
    "# 1. Create a \"Clean\" orders dataframe for delivery analysis\n",
    "# We remove orders that were never delivered/canceled to calculate accurate delivery times\n",
    "df_orders_clean = df_orders.dropna(subset=[\"order_delivered_customer_date\"])\n",
    "\n",
    "# 2. Fill nulls in 'order_approved_at' with the purchase timestamp\n",
    "# (Assumption: Instant approval if missing)\n",
    "df_orders_filled = df_orders.withColumn(\n",
    "    \"order_approved_at\",\n",
    "    when(col(\"order_approved_at\").isNull(), col(\"order_purchase_timestamp\"))\n",
    "    .otherwise(col(\"order_approved_at\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093c8a31-538b-44a2-80ef-3fe3fd1b644b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Handling Nulls in Reviews (df_order_reviews)**\n",
    "\n",
    "The Problem: review_comment_title is 88% empty and review_comment_message is 60% empty.\n",
    "\n",
    "The Logic: This is normal. People rate stars (score) but don't always write text.\n",
    "\n",
    "The Fix: Replace nulls with \"No Comment\" so your text analysis code doesn't crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e5f693-151c-47c6-8217-88dfd705776b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_reviews_clean = df_order_reviews.fillna({\n",
    "    \"review_comment_title\": \"No Title\",\n",
    "    \"review_comment_message\": \"No Message\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cfacf0-3015-44ef-a92e-26d37a9d9dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Handling Product Categories (df_products)**\n",
    "\n",
    "The Problem: product_category_name has 1.85% nulls.\n",
    "\n",
    "The Fix: Fill with \"unknown\". Also, join with df_products_category to get the English names, as the original names are likely in Portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe66206-1b21-4c4f-ad7c-4552ae000d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Fill unknown categories\n",
    "df_products_clean = df_products.fillna({\"product_category_name\": \"unknown\"})\n",
    "\n",
    "# 2. Join to get English names (Crucial for presentation)\n",
    "df_products_enriched = df_products_clean.join(\n",
    "    df_products_category, \n",
    "    \"product_category_name\", \n",
    "    \"left\"\n",
    ").select(\n",
    "    df_products_clean[\"*\"],\n",
    "    # If English name is missing (for 'unknown' category), use 'Unknown'\n",
    "    when(col(\"product_category_name_english\").isNull(), \"Unknown\")\n",
    "    .otherwise(col(\"product_category_name_english\")).alias(\"category_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5230479f-b2e5-401f-bf4e-68cd215c74eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis A: Delivery Performance (Logistics)**\n",
    "\n",
    "# Business Question: \"How long does it really take to deliver our products? Are we late?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf30e8e-98dd-4be0-a1e3-5a357e5a747f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delivery Performance Metrics"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, datediff, round, count, avg, sum\n",
    "# Calculate \"Actual Delivery Days\" vs \"Estimated Delivery Days\"\n",
    "delivery_performance = df_orders_clean.withColumn(\n",
    "    \"actual_days\", \n",
    "    datediff(col(\"order_delivered_customer_date\"), col(\"order_purchase_timestamp\"))\n",
    ").withColumn(\n",
    "    \"estimated_days\", \n",
    "    datediff(col(\"order_estimated_delivery_date\"), col(\"order_purchase_timestamp\"))\n",
    ").withColumn(\n",
    "    \"is_late\", \n",
    "    when(col(\"actual_days\") > col(\"estimated_days\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Aggregated Metrics\n",
    "delivery_metrics = delivery_performance.agg(\n",
    "    round(avg(\"actual_days\"), 2).alias(\"avg_delivery_days\"),\n",
    "    round(avg(\"estimated_days\"), 2).alias(\"avg_estimated_days\"),\n",
    "    sum(\"is_late\").alias(\"total_late_orders\"),\n",
    "    count(\"*\").alias(\"total_orders\")\n",
    ").withColumn(\n",
    "    \"late_percentage\", \n",
    "    round((col(\"total_late_orders\") / col(\"total_orders\")) * 100, 2)\n",
    ")\n",
    "\n",
    "display(delivery_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584faf43-609a-4633-8b97-165381c62a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Top Performing Product Categories (Sales)**\n",
    "\n",
    "# Business Question: \"Which categories generate the most revenue?\"\n",
    "\n",
    "Requires Joining: Order Items -> Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f3233f-6f02-4de1-983e-492dc8dc76d8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"total_revenue\":189},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769413561190}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join items with products to get category names\n",
    "revenue_by_category = df_order_items.join(df_products_enriched, \"product_id\") \\\n",
    "    .groupBy(\"category_name\") \\\n",
    "    .agg(\n",
    "        round(sum(\"price\"), 2).alias(\"total_revenue\"),\n",
    "        count(\"order_id\").alias(\"total_orders\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_revenue_formatted\", indian_format_udf(col(\"total_revenue\"))) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "display(revenue_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ff83cb-912a-48ce-b66f-f3e1a3d94ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Customer Value (RFM Proxy)**\n",
    "\n",
    "# Business Question: \"Who are our big spenders? (States)\"\n",
    "\n",
    "Requires Joining: Orders -> Order Items -> Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce61297-9596-4872-b778-9cd5e194f301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get total spend per order\n",
    "order_spend = df_order_items.groupBy(\"order_id\").agg(sum(\"price\").alias(\"order_total\"))\n",
    "\n",
    "# 2. Join with Customers to find location\n",
    "state_revenue = df_orders.join(order_spend, \"order_id\") \\\n",
    "    .join(df_customers, \"customer_id\") \\\n",
    "    .groupBy(\"customer_state\") \\\n",
    "    .agg(\n",
    "        round(sum(\"order_total\"), 2).alias(\"total_revenue\"),\n",
    "        count(\"order_id\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_revenue_formatted\", indian_format_udf(col(\"total_revenue\")))\\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "display(state_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4d81aa-844e-4635-b579-15eef08590d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Map the Sellers vs. Customers.(Geospatial Analysis)**\n",
    "\n",
    "# Business Question: \"Are sellers located near our customers, or are we shipping across the country?\"\n",
    "\n",
    "Requires Joining: df_geolocation -> Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6655a48a-1939-43f4-9902-69a4efb2f185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean Geolocation: Take the average Lat/Lng for each Zip Code to avoid duplicates\n",
    "df_geo_clean = df_geolocation.groupBy(\"geolocation_zip_code_prefix\") \\\n",
    "    .agg(\n",
    "        avg(\"geolocation_lat\").alias(\"lat\"),\n",
    "        avg(\"geolocation_lng\").alias(\"lng\")\n",
    "    )\n",
    "\n",
    "# Now you can join this 'df_geo_clean' with 'df_customers' (on zip) \n",
    "df_customers_geo = df_customers.join(\n",
    "    df_geo_clean,\n",
    "    df_customers.customer_zip_code_prefix == df_geo_clean.geolocation_zip_code_prefix,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# and 'df_sellers' (on zip) to visualize locations.\n",
    "df_sellers_geo = df_sellers.join(\n",
    "    df_geo_clean,\n",
    "    df_sellers.seller_zip_code_prefix == df_geo_clean.geolocation_zip_code_prefix,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "display(df_customers_geo.limit(5))\n",
    "display(df_sellers_geo.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c51ce6a-3b02-4019-80b6-a685bf00299f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**RFM Analysis (Customer Segmentation)**\n",
    "\n",
    "Goal: Segment customers into \"Champions,\" \"At Risk,\" and \"Hibernating\" based on their buying behavior.\n",
    "\n",
    "Recency: How many days since their last purchase?\n",
    "\n",
    "Frequency: How many total orders have they made?\n",
    "\n",
    "Monetary: How much have they spent in total?\n",
    "\n",
    "Requires joining df_orders, df_order_items, and df_customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91be959-0c31-4c92-b37f-991742424934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, countDistinct, sum, datediff, current_date, col, lit\n",
    "\n",
    "# 1. Prepare the Base Table\n",
    "# We need Customer Unique ID, Last Purchase Date, Total Orders, Total Spend\n",
    "rfm_table = df_orders.join(df_order_items, \"order_id\") \\\n",
    "    .join(df_customers, \"customer_id\") \\\n",
    "    .groupBy(\"customer_unique_id\") \\\n",
    "    .agg(\n",
    "        max(\"order_purchase_timestamp\").alias(\"last_purchase_date\"),\n",
    "        countDistinct(\"order_id\").alias(\"frequency\"),\n",
    "        sum(\"price\").alias(\"monetary\")\n",
    "    ) \\\n",
    "    .withColumn(\"recency\", datediff(current_date(), col(\"last_purchase_date\")))\n",
    "\n",
    "# 2. Score them (Simple Quantile-based scoring)\n",
    "# Create simple segments (Logic: High Recency is BAD, High Freq/Monetary is GOOD)\n",
    "rfm_scored = rfm_table.withColumn(\n",
    "    \"segment\", \n",
    "    when((col(\"recency\") < 90) & (col(\"frequency\") >= 2), \"Champion\")\n",
    "    .when((col(\"recency\") < 90) & (col(\"frequency\") == 1), \"New Customer\")\n",
    "    .when((col(\"recency\") >= 90) & (col(\"monetary\") > 500), \"Can't Lose\")\n",
    "    .otherwise(\"Hibernating\")\n",
    ")\n",
    "\n",
    "display(rfm_scored.groupBy(\"segment\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00e9449-1cd7-4dc9-8cd7-7e7518146294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cohort Analysis (Retention)**\n",
    "\n",
    "Goal: See if customers acquired in January stick around longer than those acquired in February. \n",
    "\n",
    "Metric: Retention Rate (Month 0 to Month 12).\n",
    "\n",
    "Requires--> df_orders and df_customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfc66fe-621b-436f-861c-4ee332c88795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, min, month, year\n",
    "\n",
    "# 1. Get the \"Cohort Month\" (First Purchase Month) for each customer\n",
    "customer_cohorts = df_orders.join(df_customers, \"customer_id\") \\\n",
    "    .groupBy(\"customer_unique_id\") \\\n",
    "    .agg(min(\"order_purchase_timestamp\").alias(\"first_purchase_date\")) \\\n",
    "    .withColumn(\"cohort_month\", date_format(col(\"first_purchase_date\"), \"yyyy-MM\"))\n",
    "\n",
    "# 2. Join back to get ALL activities\n",
    "cohort_analysis = df_orders.join(df_customers, \"customer_id\") \\\n",
    "    .join(customer_cohorts, \"customer_unique_id\") \\\n",
    "    .withColumn(\"activity_month\", date_format(col(\"order_purchase_timestamp\"), \"yyyy-MM\")) \\\n",
    "    .groupBy(\"cohort_month\", \"activity_month\") \\\n",
    "    .agg(countDistinct(\"customer_unique_id\").alias(\"active_users\")) \\\n",
    "    .orderBy(\"cohort_month\", \"activity_month\")\n",
    "\n",
    "# This result is best viewed in a Pivot Table in Excel/Pandas\n",
    "display(cohort_analysis)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ecommerce-Data-Analysis-with-PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
